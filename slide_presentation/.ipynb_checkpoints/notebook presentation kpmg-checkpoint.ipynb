{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Scrapping Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scraping pdf :\n",
    "\n",
    "1. scraping the 'company's id' who have publish something in the \"moniteur\" in a given periode.\n",
    "    2 output :  list of id's \n",
    "    \n",
    "    [\n",
    "    \"0691625539\",\n",
    "    \"0702777668\",\n",
    "    \"0702795187\",\n",
    "    \"0702775886\",\n",
    "    \"0702799543\",\n",
    "    \"0702768562\",\n",
    "    \"0702761832\",\n",
    "    \"0701797176\",\n",
    "    \"0701791931\",\n",
    "    \"0702764307\",\n",
    "    \"0701819843\",\n",
    "    \"0702798157\",\n",
    "    \n",
    "                list of url \n",
    "                \n",
    "    [\n",
    "    \"https://www.staatsbladmonitor.be/bedrijfsfiche.html?ondernemingsnummer=0687513729\",\n",
    "    \"https://www.staatsbladmonitor.be/bedrijfsfiche.html?ondernemingsnummer=0687518479\",\n",
    "    \"https://www.staatsbladmonitor.be/bedrijfsfiche.html?ondernemingsnummer=0687490072\",\n",
    "    \"https://www.staatsbladmonitor.be/bedrijfsfiche.html?ondernemingsnummer=0686796523\",\n",
    "    \"https://www.staatsbladmonitor.be/bedrijfsfiche.html?ondernemingsnummer=0687502742\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def dategenerator():\n",
    "    dt = datetime.datetime(2018, 1, 1)\n",
    "    end = datetime.datetime(2018, 12, 31)\n",
    "    step = datetime.timedelta(days=1)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    while dt < end:\n",
    "        result.append(dt.strftime('%Y-%m-%d'))\n",
    "        dt += step\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import json\n",
    "import re\n",
    "#from utils.date import dategenerator\n",
    "\n",
    "# launch script : scrapy runspider THISFILE.py \n",
    "\n",
    "class BlogSpider(scrapy.Spider):\n",
    "    name = 'DE STAATSBLADMONITOR 2018 '\n",
    "\n",
    "    start_urls = []\n",
    "\n",
    "    daterange = dategenerator()\n",
    "    for date in daterange:\n",
    "        start_urls.append('https://www.staatsbladmonitor.be/oprichtingen-bedrijven.html?datum='+date)\n",
    "\n",
    "\n",
    "    data_final = []\n",
    "    token = []\n",
    "\n",
    "    def parse(self, response):\n",
    "        for data in response.css('.data::text'):\n",
    "            if re.match('^[0-9]+$', data.get()):\n",
    "                self.data_final.append('https://www.staatsbladmonitor.be/bedrijfsfiche.html?ondernemingsnummer='+data.get())\n",
    "                self.token.append(data.get())\n",
    "\n",
    "        with open('link_01.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.data_final, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        with open('token.json', 'w', encoding='utf-8') as g:\n",
    "            json.dump(self.token, g, ensure_ascii=False, indent=4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.from the url's scraped in the first step , we scrap the link to the pdf from 'le moniteur'\n",
    "\n",
    "    output example :\n",
    "    \n",
    "    {\"http://www.ejustice.just.fgov.be/tsv_pdf/2018/01/08/18300807.pdf\": {\"VAT\": \"0687513729\", \"source_ref\": \"18300807\", \"source_date\": \"20180108\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-60309236a075>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0murls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'link_01_2018.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# 1000 first for testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    535\u001b[0m             )\n\u001b[0;32m    536\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'frame'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'series'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 652\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\json\\json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"columns\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m             self.obj = DataFrame(\n\u001b[1;32m--> 871\u001b[1;33m                 loads(json, precise_float=self.precise_float), dtype=None)\n\u001b[0m\u001b[0;32m    872\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"split\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m             decoded = {str(k): v for k, v in compat.iteritems(\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from fake_useragent import UserAgent \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "from requests.exceptions import SSLError\n",
    "from OpenSSL.SSL import SysCallError\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "\n",
    "urls = pd.read_json('link_01_2018.json')\n",
    "\n",
    "# 1000 first for testing\n",
    "urls = urls[0][:1000].tolist()\n",
    "\n",
    "def link_pdf():\n",
    "    ua = UserAgent()\n",
    "    urls_pdf = {}\n",
    "    for i in urls:\n",
    "        try:\n",
    "            response = requests.get(i,  headers={'User-Agent': ua.random}, verify=False)\n",
    "            print(response)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            for link in soup.findAll('a', attrs={'href': re.compile(\"pdf$\")}):\n",
    "                links = link.get('href')\n",
    "                #urls_pdf.append([link.get('href'), i[-10:]])\n",
    "                urls_pdf[link.get('href')] = {'VAT': i[-10:],\n",
    "                                              'source_ref': links[-12:-4],\n",
    "                                              'source_date': str(links[-23:-19])+str(links[-18:-16])+str(links[-15:-13])\n",
    "                                              }\n",
    "\n",
    "        except SysCallError :\n",
    "            print(f\"SysCallError for {i[-10]}\")\n",
    "\n",
    "\n",
    "    return urls_pdf\n",
    "\n",
    "result = link_pdf()\n",
    "\n",
    "with open('link_pdf_2018.json', 'w') as outfile:\n",
    "    json.dump(result, outfile)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. downloading pdf , name pdf = yyyyMMDD_VATNUM.pdf        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import time\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import sys\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "print('Beginning file download with wget module')\n",
    "\n",
    "#urlstest = \"http://www.ejustice.just.fgov.be/tsv_pdf/2019/01/03/19300336.pdf\"\n",
    "\n",
    "with open('link_pdf_2018.json', 'r') as f:\n",
    "        urls = json.load(f)\n",
    "\n",
    "ua = UserAgent()\n",
    "\n",
    "for key in urls :\n",
    "    try:\n",
    "        time.sleep(0.1)\n",
    "        print(f'trying: {urls[key]}')\n",
    "        pdfname = str(urls[key]['source_date'])+'_'+str(urls[key]['VAT'])\n",
    "        #print(pdfname)\n",
    "        \n",
    "        if os.path.exists('./pdf/'+str(pdfname)+'.pdf') is False:\n",
    "\n",
    "            myfile = requests.get(key, headers={'User-Agent': ua.random})\n",
    "            print(f'yo : {key}')\n",
    "            with open(f'pdf/{pdfname}.pdf', 'wb') as file:\n",
    "                file.write(myfile.content)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    except:\n",
    "        print(f'ERROR url: {key}')\n",
    "        print(sys.exc_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text extraction:\n",
    "\n",
    "Then, \n",
    "- we extract the pdf content:\n",
    "\n",
    "    - we choose to convert the pdf's to images :\n",
    "        ADD IMAGE HERE\n",
    "    - crop the iamges \n",
    "\n",
    "    - and extract the text by OCR using tesseract and pytesseract\n",
    "\n",
    "\n",
    "- determine the language :\n",
    "\n",
    " with LanguageDetector from spacy_langdetect library\n",
    " \n",
    " \n",
    "- scrap the object of the document from 'le moniteur' \n",
    " \n",
    "- fill a mongodb  with :\n",
    "\n",
    "    _id : VAT\n",
    "    \n",
    "    documents :{YYYYMMDD: {'text' : EXTRACTED TEXT , 'object': SCRAPED OBJECT }}\n",
    "    \n",
    "    language : extracted language\n",
    "    \n",
    "    \n",
    "NB : text and cropped images are save in the filesystem    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import spacy\n",
    "from pymongo.errors import DuplicateKeyError\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from pymongo import MongoClient\n",
    "from wand.image import Image as WandImg\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from scrap_act_object_2 import act_obj\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\sebch\\AppData\\Local\\Tesseract-OCR\\tesseract.exe'\n",
    "#SEB path  r'C:\\Users\\sebch\\AppData\\Local\\Tesseract-OCR\\tesseract.exe'\n",
    "#original path  r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "### German, nederlands and frech Tesseract packages can be download from  : https://github.com/tesseract-ocr/tesseract/wiki/Data-Files\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(LanguageDetector(), name=\"language_detector\", last=True)\n",
    "\n",
    "def pdfsToTxt(folderPath):\n",
    "    files = []\n",
    "\n",
    "    for r, d, f in os.walk(folderPath):\n",
    "        for file in f:\n",
    "            if \".pdf\" in file:\n",
    "                files.append(os.path.join(r, file))\n",
    "\n",
    "    for f in files:\n",
    "        existing_txts = glob.glob(format_txt_filename(f, \"dummy\").replace(\"dummy\", \"*\"))\n",
    "        if existing_txts :\n",
    "            print(\"TXT file already exists, skipping text detection and db insertion\")\n",
    "        else:\n",
    "            #extract text from images\n",
    "            image_path_list = convert_to_images(f, 300)\n",
    "            total_page_count = str(len(image_path_list))\n",
    "            page_count = 0\n",
    "            pdf_string = \"\"\n",
    "            for image_path in image_path_list :\n",
    "                page_count += 1\n",
    "                pdf_img = Image.open(image_path)\n",
    "                pdf_string += pytesseract.image_to_string(pdf_img, lang='fra+deu+nld')\n",
    "                print(\"Processed page \" + str(page_count) + \"/\" + total_page_count\n",
    "                      + \" (\" + image_path + \")\")\n",
    "\n",
    "            #detect language\n",
    "            doc = nlp(pdf_string)\n",
    "            language = doc._.language[\"language\"]\n",
    "\n",
    "            #write txt\n",
    "            txt_file_path = format_txt_filename(f, language)\n",
    "            txt_file = open(txt_file_path, \"w+\", encoding='utf-8')\n",
    "            txt_file.write(pdf_string)\n",
    "            print(\"Txt created: \" + txt_file_path)\n",
    "            txt_file.close()\n",
    "\n",
    "            #write to db\n",
    "            vat_nr_date = os.path.basename(f).replace(\".pdf\", \"\")\n",
    "            uid = vat_nr_date[-10:]\n",
    "            date = vat_nr_date[-19:-11]\n",
    "            statute_dict = {\"_id\" : uid,\n",
    "                            \"documents\": {f'{date}': {'text' : pdf_string, 'object': act_obj(uid[1:], date)}},\n",
    "                            \"language\" : language}\n",
    "            client = MongoClient()\n",
    "            db = client.kpmg\n",
    "            stat_coll = db.statutes\n",
    "            stat_doc = stat_coll.find_one({\"_id\": uid})\n",
    "            if stat_doc:\n",
    "                stat_coll.update_one({'_id': uid},\n",
    "                                      {'$set': {f\"documents.{date}\": {'text': pdf_string, 'object': act_obj(uid[1:], date)}}},\n",
    "                                      upsert=True\n",
    "                                      )\n",
    "            else:\n",
    "                insert_result = stat_coll.insert_one(statute_dict)\n",
    "                if insert_result.acknowledged:\n",
    "                    print(\"Document inserted into db kmpg, collection statutes, with _id \" + str(\n",
    "                        insert_result.inserted_id))\n",
    "\n",
    "def encode_newlines(text) :\n",
    "    return text.replace(\"\\\\r\\\\n\", \"\\r\\n\").replace(\"\\\\n\", \"\\n\").replace(\"\\\\r\", \"\\r\")\n",
    "\n",
    "def remove_newlines(text) :\n",
    "    return text.replace(\"\\\\r\\\\n\", \" \").replace(\"\\\\n\", \" \").replace(\"\\\\r\", \" \")\n",
    "\n",
    "def format_txt_filename(original_pdf, language) :\n",
    "    return os.path.abspath(original_pdf).replace(\".pdf\", \"_\").replace(\"\\\\pdf\", \"\\\\txt\") + \\\n",
    "    language + \\\n",
    "    \".txt\"\n",
    "\n",
    "def convert_to_images(file, resolution) :\n",
    "    file_path = os.path.abspath(file)\n",
    "    img_path_template = file_path.replace(\"\\\\pdf\", \"\\\\png\").replace(\".pdf\", \"\")\n",
    "\n",
    "    image_paths = glob.glob(img_path_template + \"*\")\n",
    "    if image_paths :\n",
    "        print(\"Images for \" + file_path + \" already exist, skipping image conversion\")\n",
    "    else :\n",
    "        with WandImg(filename=file_path, resolution=resolution) as source:\n",
    "            source.compression_quality = 99\n",
    "            images = source.sequence\n",
    "            nr_of_pages = len(images)\n",
    "            image_paths = []\n",
    "            for i in range(nr_of_pages):\n",
    "                img_path = img_path_template + \"_\" + str(i) + \".png\"\n",
    "                #croping pages (if first pages else the others)\n",
    "                if i == 0:\n",
    "                    images[i].crop(int(images[i].size[0] * 0.16), int(images[i].size[1] * 0.20), int(images[i].size[0] * 0.95),\n",
    "                                int(images[i].size[1] * 0.92))\n",
    "                else:\n",
    "                    images[i].crop(int(images[i].size[0] * 0.16), int(images[i].size[1] * 0.052), int(images[i].size[0] * 0.95),\n",
    "                                int(images[i].size[1] * 0.92))\n",
    "                WandImg(images[i]).save(filename=img_path)\n",
    "                image_paths.append(img_path)\n",
    "            print(\"Image(s) created: \" + str(image_paths))\n",
    "\n",
    "    return image_paths\n",
    "\n",
    "\n",
    "pdfsToTxt(\".\\\\pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scraping of the text object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "\n",
    "def act_obj(uid, date):\n",
    "    date = str(date)\n",
    "    date_f = str(date)[:4]+'-'+str(date)[4:6]+'-'+str(date[6:8])\n",
    "\n",
    "    url = f\"https://www.ejustice.just.fgov.be/cgi_tsv/tsv_l_1.pl?lang=fr&sql=btw+contains+%27{uid}%27&fromtab=TSV&rech=1&pdda=&pddm=&pddj=&pdfa=&pdfm=&pdfj=&naam=&postkode=&localite=&numpu=&hrc=&akte=&btw={uid}&jvorm=&land=&set2=&set3=\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.text,features=\"lxml\").find_all('td')\n",
    "    for i in range(0, round(len(soup) / 2) - 1):\n",
    "        test02 = soup[-2 + (-3 * i)].find_all('br')\n",
    "        test03 = test02[-1].next_sibling\n",
    "        if date_f in test03:\n",
    "            result = test02[-1].previous_sibling\n",
    "            return result\n",
    "\n",
    "#print(act_obj(687501257, 20180108))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Let's then take all the metadata we can get from the governement website and fill the db with.\n",
    "\n",
    "\n",
    "        for this we use a reusable set of csv (https://economie.fgov.be/fr/themes/entreprises/banque-carrefour-des/services-pour-tous/banque-carrefour-des-2 ) update every month.\n",
    "        \n",
    "        rem : we use pandas to recover the data , for sure some better way are possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## need the csv and the token list generate in the first step\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import json\n",
    "from dictionaries import load_dict\n",
    "from meta_gen import dict_gen\n",
    "\n",
    "print('loading tables')\n",
    "\n",
    "activity = pd.read_csv('./OpenData/activity.csv')\n",
    "table02 = pd.read_csv('./OpenData/address.csv')\n",
    "table03 = pd.read_csv('./OpenData/code.csv')\n",
    "table04 = pd.read_csv('./OpenData/contact.csv')\n",
    "table05 = pd.read_csv('./OpenData/denomination.csv')\n",
    "table06 = pd.read_csv('./OpenData/enterprise.csv')\n",
    "table07 = pd.read_csv('./OpenData/establishment.csv')\n",
    "\n",
    "\n",
    "print('tables loaded')\n",
    "#need probably to be re-scraped to match with the pdf's\n",
    "vat_numbers = pd.read_json('token_test.json')\n",
    "\n",
    "#TODO for testing purpose - remove\n",
    "#vat_numbers = vat_numbers[:50]\n",
    "\n",
    "vat_numbers = \"0\" + vat_numbers.astype(str)\n",
    "##formatting TVA number not needed here anymore\n",
    "#vat_numbers[1]= vat_numbers[0].str[:4] + '.' + vat_numbers[0].str[4:7] + \\\n",
    "#                '.'+vat_numbers[0].str[7:10]\n",
    "vat_uids = vat_numbers[0].tolist()\n",
    "#vat_formatted = vat_numbers[1].tolist()\n",
    "\n",
    "print(vat_uids)\n",
    "#print(vat_formatted)\n",
    "\n",
    "\n",
    "print('loading dictionaries')\n",
    "ActivityGroup_dic,  JuridicalForm_dic, JuridicalSituation_dic, Nace2003_dic, Nace2008_dic,TypeOfEnterprise_dic = load_dict(table03)\n",
    "\n",
    "\n",
    "def update_statutes_in_db(uids):\n",
    "    client = MongoClient()\n",
    "    db = client.kpmg\n",
    "    stat_coll = db.statutes\n",
    "    for uid in uids :\n",
    "        stat_doc = stat_coll.find_one({\"_id\": uid})\n",
    "        if stat_doc :\n",
    "            meta_dic = dict_gen(uid, activity, table02, table04, table05, table06, table07,\n",
    "                ActivityGroup_dic,  JuridicalForm_dic, JuridicalSituation_dic, Nace2003_dic, Nace2008_dic, TypeOfEnterprise_dic )\n",
    "            meta_dic.update(stat_doc) #this way fields in stat_doc do not get overwritten\n",
    "            stat_coll.update_one({'_id': uid}, {\"$set\": meta_dic}, upsert=False)\n",
    "            print(\"Found statute with uid \" + uid + \". Updated meta data where empty\")\n",
    "\n",
    "        else :\n",
    "            print(\"Document with uid \" + str(uid) + \" not found in database\")\n",
    "\n",
    "\n",
    "update_statutes_in_db(vat_uids)\n",
    "\n",
    "# this part generate the meta data for one given VAT NUMBER\n",
    "#very slow ...\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#from scrap_act_object import act_obj\n",
    "def dict_gen(uid, activity, table02, table04, table05, table06, table07,\n",
    "             ActivityGroup_dic, JuridicalForm_dic, JuridicalSituation_dic, Nace2003_dic, Nace2008_dic, TypeOfEnterprise_dic):\n",
    "\n",
    "    meta_dic = dict()\n",
    "    uid_f = str(uid[:4]+'.'+str(uid[4:7])+'.'+str(uid[7:10]))\n",
    "    uid = int(uid)\n",
    "    # formatting 2 features from table 06 to match with the code table (table03)\n",
    "\n",
    "    temp = table06.JuridicalForm.fillna(0).astype(int).astype(str)\n",
    "    temp2 = table06.JuridicalSituation.fillna(0).astype(int).astype(str)\n",
    "    temp = [\"0\" + i if len(i) == 2 else \"00\" + i if len(i) == 1 else i for i in temp]\n",
    "    temp2 = [\"0\" + i if len(i) == 2 else \"00\" + i if len(i) == 1 else i for i in temp2]\n",
    "    table06.JuridicalForm = temp\n",
    "    table06.JuridicalSituation = temp2\n",
    "\n",
    "    table06_row = table06[table06['EnterpriseNumber'] == uid_f]\n",
    "    table05_row = table05[table05['EntityNumber'] == uid_f]\n",
    "    table05_row_abbr = table05[(table05['EntityNumber'] == uid_f) & (table05.TypeOfDenomination == 2)]\n",
    "    table01_row_main = activity[(activity['EntityNumber'] == uid_f) & (activity.Classification == 'MAIN')].astype(str)\n",
    "    table01_row_sec = activity[(activity['EntityNumber'] == uid_f) & (activity.Classification == 'SECO')].astype(str)\n",
    "    table07_row = table07[table07['EnterpriseNumber'] == uid_f]\n",
    "    table02_row = table02[table02['EntityNumber'] == uid_f]\n",
    "    table04_row_mail = table04[(table04['EntityNumber'] == uid_f) & (table04['ContactType'] == 'EMAIL')]\n",
    "    table04_row_phone = table04[(table04['EntityNumber'] == uid_f) & (table04['ContactType'] == 'TEL')]\n",
    "\n",
    "    if len(table06_row) == 0:\n",
    "        print(\"No data found for vat number \" + uid_f + \" in the CSVs\")\n",
    "    else:\n",
    "        meta_dic['VAT Number'] = table06_row.iloc[0]['EnterpriseNumber']\n",
    "\n",
    "        #meta_dic['act object'] = act_obj(uid)\n",
    "\n",
    "        meta_dic['Denomination'] = table05_row.iloc[0]['Denomination']\n",
    "\n",
    "        meta_dic['Abbr'] = (lambda x: table05_row_abbr.iloc[0]['Denomination']\n",
    "        if len(table05_row_abbr) != 0 else 'NONE')(uid_f)\n",
    "\n",
    "        meta_dic['ActivityGroup'] = (lambda x: table01_row_main.iloc[0]['ActivityGroup']\n",
    "        if len(table01_row_main) != 0 else \"none\")(uid_f)\n",
    "\n",
    "        meta_dic['Main activity'] = (\n",
    "            lambda x: [table01_row_main.iloc[0]['NaceVersion'], table01_row_main['NaceCode'].unique().tolist()]\n",
    "            if len(table01_row_main) != 0 else \"none\")(uid_f)\n",
    "\n",
    "        meta_dic['Secondary activity'] = (lambda x: table01_row_sec['NaceCode'].unique().tolist()\n",
    "        if len(table01_row_sec) != 0 else \"none\")(uid_f)\n",
    "\n",
    "        meta_dic['Foundation Date'] = table06_row['StartDate'].iloc[0]\n",
    "\n",
    "        meta_dic['Establishment #'] = len(table07_row)\n",
    "\n",
    "        meta_dic['Establishment StartDate'] = table07_row['StartDate'].to_list()\n",
    "\n",
    "        meta_dic['JuridicalSituation'] = str(table06_row.iloc[0]['JuridicalSituation'])\n",
    "\n",
    "        meta_dic['TypeOfEnterprise'] = str(table06_row.iloc[0]['TypeOfEnterprise'])\n",
    "\n",
    "        meta_dic['JuridicalForm'] = str(table06_row.iloc[0]['JuridicalForm'])\n",
    "\n",
    "        # meta_dic['Language']= table06_row.iloc[0]['TypeOfEnterprise']\n",
    "\n",
    "        meta_dic['Zipcode'] = table02_row.iloc[0]['Zipcode']\n",
    "\n",
    "        meta_dic['Street'] = table02_row.iloc[0]['StreetFR']\n",
    "\n",
    "        meta_dic['HouseNumber'] = table02_row.iloc[0]['HouseNumber']\n",
    "\n",
    "        meta_dic['Mail'] = (lambda x: table04_row_mail.iloc[0]['Value']\n",
    "        if len(table04_row_mail) != 0 else 'NA')(uid_f)\n",
    "\n",
    "        meta_dic['Phone'] = (lambda x: table04_row_phone.iloc[0]['Value']\n",
    "        if len(table04_row_phone) != 0 else 'NA')(uid_f)\n",
    "        \n",
    "        \n",
    "        # \"translation of the codes present in the csv\"\n",
    "        \n",
    "\n",
    "        for KEY, VALUE in ActivityGroup_dic.items():\n",
    "            if meta_dic['ActivityGroup'] == KEY:\n",
    "                meta_dic['ActivityGroup'] = VALUE\n",
    "\n",
    "        for KEY, VALUE in JuridicalForm_dic.items():\n",
    "            if meta_dic['JuridicalForm'] == KEY:\n",
    "                meta_dic['JuridicalForm'] = VALUE\n",
    "\n",
    "        for KEY, VALUE in JuridicalSituation_dic.items():\n",
    "            if meta_dic['JuridicalSituation'] == KEY:\n",
    "                meta_dic['JuridicalSituation'] = VALUE\n",
    "\n",
    "        for KEY, VALUE in TypeOfEnterprise_dic.items():\n",
    "            if meta_dic['TypeOfEnterprise'] == KEY:\n",
    "                meta_dic['TypeOfEnterprise'] = VALUE\n",
    "\n",
    "        if meta_dic['Main activity'][0] == 2003:\n",
    "            for KEY, VALUE in Nace2003_dic.items():\n",
    "                for i in range(0, len(meta_dic['Main activity'])):\n",
    "                    if meta_dic['Main activity'][1][i] == KEY:\n",
    "                        meta_dic['Main activity'][1][i] = (KEY, VALUE)\n",
    "\n",
    "                for i in range(0, len(meta_dic['Secondary activity'])):\n",
    "                    if meta_dic['Secondary activity'][i] == KEY:\n",
    "                        meta_dic['Secondary activity'][i] = (KEY, VALUE)\n",
    "        else:\n",
    "            for KEY, VALUE in Nace2008_dic.items():\n",
    "                for i in range(0, len(meta_dic['Main activity'][1])):\n",
    "                    if meta_dic['Main activity'][1][i] == KEY:\n",
    "                        meta_dic['Main activity'][1][i] = (KEY, VALUE)\n",
    "\n",
    "                for i in range(0, len(meta_dic['Secondary activity'])):\n",
    "                    if meta_dic['Secondary activity'][i] == KEY:\n",
    "                        meta_dic['Secondary activity'][i] = (KEY, VALUE)\n",
    "\n",
    "        #print(meta_dic)\n",
    "\n",
    "    return(meta_dic)\n",
    "\n",
    "\n",
    "\n",
    "#we need this to \"translate\" code in readable information\n",
    "\n",
    "def load_dict(table03):\n",
    "    trad = table03[['Category', 'Code', 'Description']][table03.Language == 'FR']\n",
    "    dict_list = []\n",
    "    for cat in trad.Category.unique():\n",
    "        temp_trad = trad[trad.Category == cat]\n",
    "        keys = temp_trad['Code'].to_list()\n",
    "        values = temp_trad['Description'].to_list()\n",
    "        dict_list.append(dict(zip(keys, values)))\n",
    "\n",
    "    ActivityGroup_dic = dict_list[0]\n",
    "    # Classification_dic = dict_list[1]\n",
    "    # ContactType_dic = dict_list[2]\n",
    "    # EntityContact_dic = dict_list[3]\n",
    "    JuridicalForm_dic = dict_list[4]\n",
    "    JuridicalSituation_dic = dict_list[5]\n",
    "    # Language_dic = dict_list[6]\n",
    "    Nace2003_dic = dict_list[7]\n",
    "    Nace2008_dic = dict_list[8]\n",
    "    #Status_dic = dict_list[9]\n",
    "    # TypeOfAddress_dic = dict_list[10]\n",
    "    # TypeOfDenomination_dic = dict_list[11]\n",
    "    TypeOfEnterprise_dic = dict_list[12]\n",
    "    return ActivityGroup_dic,  JuridicalForm_dic, JuridicalSituation_dic,\\\n",
    "           Nace2003_dic, Nace2008_dic, TypeOfEnterprise_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. we manage to be able to split the text into articles \n",
    "\n",
    "\n",
    "in the condition le script can find some key word in the text\n",
    "\n",
    "\n",
    "we integrate the traduction in this part too even if it make the code less readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "from googletrans import Translator\n",
    "from json.decoder import JSONDecodeError\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "client = MongoClient()\n",
    "db = client.kpmg\n",
    "stat_coll = db.statutes\n",
    "\n",
    "#key word to be found in the text:\n",
    "key_word = [\"Art\", \"Article\", 'Art.', 'Art,', 'Ast', 'Ari', \"art\", \"At\", \"ARTICLE\", \"ART\", 'Ant', 'Artikel', 'ARTIKEL', 'article', 'Articdle']\n",
    "#VAT number list\n",
    "filename = r'C:\\Users\\sebch\\Desktop\\kpmg\\GitHub Repo\\KPMG_project\\scraping_final\\scraping_staatblad_seb\\token_test.json'\n",
    "with open(filename, 'r') as f:\n",
    "    uids = json.load(f)\n",
    "\n",
    "#uids= uids[:10]\n",
    "log = []\n",
    "\n",
    "\n",
    "for uid in uids:\n",
    "    try:\n",
    "        # fetching cie first document data\n",
    "        print(f'trying {uid}')\n",
    "        stat_doc = stat_coll.find_one({\"_id\": uid})\n",
    "        doc_data = stat_doc['documents']\n",
    "        date = [i for i in doc_data][0]\n",
    "\n",
    "        #checking if it is a consitution doc\n",
    "        obje = doc_data[date]['object']\n",
    "        constitution = ['CONSTITUTION', 'OPRICHTING', 'STATUTS', 'STATUTEN']\n",
    "        for obj in obje.split():\n",
    "            if obj in constitution:\n",
    "\n",
    "                #look for key_word in text splitted\n",
    "                text = next(iter(next(iter(doc_data.values())).values()))\n",
    "                \n",
    "                #translate the full text and add to the db\n",
    "                \n",
    "                try:\n",
    "                    translation = translator.translate(text, dest='en')\n",
    "                    stat_coll.update_one({'_id': uid},\n",
    "                                         {'$set':\n",
    "                                              {f\"documents.{date}.translation\": translation.text}\n",
    "                                         },\n",
    "                                         upsert=True\n",
    "                                         )\n",
    "                    print(\"traduction added\")\n",
    "                except JSONDecodeError:\n",
    "                    print('unable to translate text')\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                # split the text by \"lignes\"\n",
    "                split = text.split('\\n')\n",
    "                #index of each ligne containing one the key_word\n",
    "                cut_index = []\n",
    "                for group in split:\n",
    "                    #removing useless characters and splitting by 'words'\n",
    "                    for word in group.translate({ord(i): ' ' for i in \":[â€˜'(),. \"}).split():\n",
    "                        if word in key_word:\n",
    "                            cut_index.append(split.index(group))\n",
    "                            \n",
    "                            \n",
    "                # text until first occurence of on e key_word\n",
    "                start = split[:cut_index[0] - 1]\n",
    "                #between two occurences\n",
    "                articles = []\n",
    "                for i in cut_index:\n",
    "                    if i != cut_index[-1]:\n",
    "                        articles.append(split[i:cut_index[cut_index.index(i) + 1]])\n",
    "                #after the last occurences\n",
    "                end = split[cut_index[-1] + 1:]\n",
    "\n",
    "                sections = [start] + articles + [end]\n",
    "                #avoiding list of several string (need to be fine tuned)\n",
    "                for element in sections:\n",
    "                    temp = str()\n",
    "                    for el in element:\n",
    "                        temp += el\n",
    "\n",
    "                    sections[sections.index(element)] = temp\n",
    "                    \n",
    "                #injection in the db    \n",
    "                stat_coll.update_one({'_id': uid},\n",
    "                                     {'$addToSet': {\n",
    "                                         f\"documents.{date}.sections\": {str(sections.index(element)): element for element in\n",
    "                                                                        sections}\n",
    "                                     }},\n",
    "                                     upsert=True\n",
    "                                     )\n",
    "                #translation articles by articles\n",
    "                try:\n",
    "                    sections_t = [translator.translate(i, dest='en').text for i in sections]\n",
    "                    print(sections_t)\n",
    "                    stat_coll.update_one({'_id': uid},\n",
    "                                         {'$addToSet': {\n",
    "                                             f\"documents.{date}.sections\": {str(sections_t.index(element)): element for\n",
    "                                                                            element in\n",
    "                                                                            sections_t}\n",
    "                                         }},\n",
    "                                         upsert=True\n",
    "                                         )\n",
    "                except JSONDecodeError :\n",
    "                    print('unable to translate articles')\n",
    "\n",
    "                log.append(uid)\n",
    "                print(f'added {uid}')\n",
    "            else :\n",
    "                pass\n",
    "    except TypeError:\n",
    "        print(f'no document found for {uid}')\n",
    "    except IndexError:\n",
    "        print(f'cannot parse document for {uid}')\n",
    "    except AttributeError:\n",
    "        print(f'cannot find document object for {uid}')\n",
    "\n",
    "with open('log.json', 'w', encoding='utf-8') as g:\n",
    "    json.dump(log, g, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
